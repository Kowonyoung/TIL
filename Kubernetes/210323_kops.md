# kops

kops (Kubernetes Operation): 클라우드 환경에서 쿠버네티스 생성 및 관리를 쉽게 하도록 도와주는 오픈소스 툴

<br>

# 1. 작업환경 구축

### 가상머신 준비

AWS EC2 인스턴스 생성 

- Ubuntu 18.04 AMI 선택
- 스토리지 크기 : 20
- 태그 : Name - k8s-ec2
- 보안그룹 : k8s-security-group, 내 IP만 허용
- 새 키 페어 생성 : k8s-ec2

![image](https://user-images.githubusercontent.com/77096463/112075064-1016b180-8bbb-11eb-9609-f2f5fe41e979.png)

<br>

탄력적 IP 주소 생성하여 k8s-ec2 인스턴스와 연결 > 재연결 시 퍼블릭 ip주소가 계속 바뀌기 때문에

![image](https://user-images.githubusercontent.com/77096463/112075207-65eb5980-8bbb-11eb-9e8d-9ddd9758886a.png)

<br>

### 인스턴스 접속 (xshell)

[cmd] ubuntu 18.04 AMI는 ubuntu로 사용자가 정해져 있음

```
C:\Users\Lenovo>cd C:\Users\Lenovo\Downloads

C:\Users\Lenovo\Downloads>ssh -i k8s-ec2.pem ubuntu@18.xx.xx.xx
The authenticity of host '18.xx.xx.xx (18.xx.xx.xx)' can't be established.
ECDSA key fingerprint is SHA256:CQCpxXqXXEofODPIlXoos5JWZOJaZS+9GsSs7ieQzjg.
Are you sure you want to continue connecting (yes/no)? yes
```

<br>

만일 접속이 안된다면 .ssh 라는 히든 폴더로 이동하여 known_hosts 내용 지운 다음 위의 명령어 재수행

```
C:\Users\Lenovo>cd .ssh
```

<br>

[xshell 연결 탭]

- 이름 : EC2-kops
- 호스트 : 인스턴스 퍼블릭 IPV4 주소
- 포트 번호 : 22

[xshell 사용자 인증 탭]

- 사용자 이름 : ubuntu
- 방법 : Public Key > k8s-ec2 키 페어 가져오기 후 선택

![image](https://user-images.githubusercontent.com/77096463/112076499-df844700-8bbd-11eb-9884-4f056692ee02.png)

<br>

### kops 설치

만일 wget이 없다면 `apt-get install -y wget`

```
$ wget -O kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
$ chmod +x ./kops
$ sudo mv ./kops /usr/local/bin/kops
```

<br>

### kubectl 설치

```
$ wget -O kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl
```

<br>

### IAM - 그룹 생성, 사용자 생성

그룹 생성

- 이름 : k8s-group
- 정책 : 5가지 선택 (아래 화면 참고)

![image](https://user-images.githubusercontent.com/77096463/112079009-ca5de700-8bc2-11eb-8bac-6e59727406d7.png)

<br>

사용자 추가

- 사용자 이름 : k8s-user
- 액세스 유형 : 프로그래밍 방식 액세스 / AWS Management Console 액세스
- 그룹 k8s-group 선택
- 태그 : name - k8s-users
- 액세스 키  ID, 비밀 액세스 키 반드시 다운받기

![image](https://user-images.githubusercontent.com/77096463/112079465-8c14f780-8bc3-11eb-835c-00ab020468fd.png)

<br>

### AWS CLI 설치 후 설정

`aws configure` 명령어 입력 후 다운받은 사용자 액세스 키 ID, 비밀 액세스 키 값 입력 

```
$ sudo apt update
$ sudo apt-get install -y python3-pip
$ sudo apt install awscli

$ aws configure
AWS Access Key ID [None]: AKIXXXXXXXXXXXXXXXXXX
AWS Secret Access Key [None]: /jyrXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
Default region name [None]: us-east-1
Default output format [None]: 

$ aws ec2 describe-instances
$ aws iam list-users
{
    "Users": [
        {
            "Path": "/",
            "UserName": "k8s-user",
            "UserId": "AIDATXJSWD4OSGUD5BY7O",
            "Arn": "arn:aws:iam::256193732381:user/k8s-user",
            "CreateDate": "2021-03-23T01:34:43Z"
        }
    ]
}
```

<br>



# 2. 클러스터 생성

### S3 버킷 생성

```
$ aws s3api create-bucket --bucket haerim --region us-east-1
{
    "Location": "/haerim"
}

$ aws s3api put-bucket-versioning --bucket haerim --versioning-configuration Status=Enabled
```

<br>

### 환경 변수 설정

```
$ export AWS_ACCESS_KEY_ID=$<access_ID>
$ export AWS_SECRET_ACCESS=$<secret_access_key>
$ export NAME=haerimcluster.k8s.local
$ export KOPS_STATE_STORE=s3://haerim
```

<br>

### SSH Key Pair 생성

rsa 알고리즘을 사용하는 키페어 생성

```
$ ssh-keygen -t rsa

$ ls -al /home/ubuntu/.ssh/
total 20
drwx------ 2 ubuntu ubuntu 4096 Mar 23 02:20 .
drwxr-xr-x 7 ubuntu ubuntu 4096 Mar 23 01:42 ..
-rw------- 1 ubuntu ubuntu  389 Mar 23 00:33 authorized_keys
-rw------- 1 ubuntu ubuntu 1679 Mar 23 02:20 id_rsa
-rw-r--r-- 1 ubuntu ubuntu  405 Mar 23 02:20 id_rsa.pub
```

<br>

### 사용 가능한 AZ 확인

```
$ aws ec2 describe-availability-zones --region us-east-1
{
    "AvailabilityZones": [
        {
            "State": "available",
            "OptInStatus": "opt-in-not-required",
            "Messages": [],
            "RegionName": "us-east-1",
            "ZoneName": "us-east-1a",
            "ZoneId": "use1-az1",
            "GroupName": "us-east-1",
            "NetworkBorderGroup": "us-east-1",
            "ZoneType": "availability-zone"
        },
        {
...
```

<br>

### 클러스터 생성을 위한 AZ 지정

ig : instance group

```
$ kops create cluster --zones us-east-1a ${NAME}
$ kops edit cluster ${NAME}
$ kops get ig --name ${NAME}
NAME			ROLE	MACHINETYPE	MIN	MAX	ZONES
master-us-east-1a	Master	t3.medium	1	1	us-east-1a
nodes-us-east-1a	Node	t3.medium	1	1	us-east-1a
```

<br>

### 마스터 노드 확인, 노드 수 조절

Master : 1개, Node는 3개 정도로 조절

- Master 갯수에 따라 EC2 인스턴스의 갯수도 증가 및 감소하니 조심하기
- machineType : t2.medium설정

```
$ kops edit ig master-us-east-1a --name ${NAME}
$ kops edit ig nodes-us-east-1a --name ${NAME}
```

![image](https://user-images.githubusercontent.com/77096463/112084617-748e3c80-8bcc-11eb-8f0e-b47904af2954.png)

<br>

### 클러스터 생성

```
$ kops update cluster ${NAME} --yes
```

명령어 수행 후 콘솔창에서 인스턴스 상태를 확인하면 설정한 마스터 노드 1개와 워커노드 3개가 생성되었음을 확인

![image](https://user-images.githubusercontent.com/77096463/112085422-d4d1ae00-8bcd-11eb-9cc0-a793c9aa9ccc.png)

<br>

:rotating_light:**트러블슈팅 - Validation failed: unexpected error during validation: error listing nodes: Unauthorized**

> `kops validate cluster` 명령어 입력 시 위와 같이 unauthroized 오류가 나는데 이는 기존의 클러스터를 지우고 새로운 클러스터 생성 시 kubeconfig에 예전 계정이 남아있어 kOps/kubectl가 기존 계정을 다시 사용하기에 나는 오류이다. 
>
> `kops export kubecfg ${NAME} --admin` 명령어 입력 후  `kops validate cluster` 실행하면 오류 해결~! 

도메인 등록

```
$ kops export kubecfg ${NAME} --admin
```



### 클러스터 테스트

클러스터 생성하고 마스터, 워커 노드 설정하는 과정에서 시간이 걸릴 수 있음

```
$ kops validate cluster
Using cluster from kubectl context: haerimcluster.k8s.local

Validating cluster haerimcluster.k8s.local

INSTANCE GROUPS
NAME			ROLE	MACHINETYPE	MIN	MAX	SUBNETS
master-us-east-1a	Master	t2.micro	1	1	us-east-1a
nodes-us-east-1a	Node	t2.micro	3	3	us-east-1a

NODE STATUS
NAME				ROLE	READY
ip-172-20-32-19.ec2.internal	node	True
ip-172-20-32-54.ec2.internal	node	True
ip-172-20-34-254.ec2.internal	master	True
ip-172-20-62-225.ec2.internal	node	True

Your cluster haerimcluster.k8s.local is ready
```

<br>

중간중간 자꾸 아래와 같은 validation error가 나오는데 10분 정도 기다리니 해결됐다

```
VALIDATION ERRORS
KIND	NAME									MESSAGE
Pod	kube-system/kube-controller-manager-ip-172-20-34-254.ec2.internal	system-cluster-critical pod "kube-controller-manager-ip-172-20-34-254.ec2.internal" is not ready (kube-controller-manager)
Pod	kube-system/kube-scheduler-ip-172-20-34-254.ec2.internal		system-cluster-critical pod "kube-scheduler-ip-172-20-34-254.ec2.internal" is not ready (kube-scheduler)

Validation Failed

Validation failed: cluster not yet healthy
```

<br>

### 클러스터 삭제

```
$ kops delete cluster --name ${NAME} --yes
```
<br>
-------------

# Deployment

### 1. 로컬 docker에서 테스트 - 이미지 가져오기

도커 이미지 가져오기 

```
[root@node2 ~]# docker pull edowon0623/my-user-service
[root@node2 ~]# docker run -d -p 8080:8080 edowon0623/my-user-service
[root@node2 ~]# docker ps | grep user
d40f9d5eb1e1   edowon0623/my-user-service   "java -jar UserServi…"   41 seconds ago   Up 29 seconds   0.0.0.0:8080->8080/tcp   optimistic_bohr
```

<br>

postman 열기

- GET 방식으로 http://127.0.0.1:28080/users 호출
- 현재 node2에서 컨테이너 실행 중인데 vagrantfile보면 `  cfg.vm.network "forwarded_port", guest: 8080, host: 28080` 코드와 같이 8080번은 28080번으로 포트포워딩 돼있으므로 위의 주소로 호출

![image](https://user-images.githubusercontent.com/77096463/112110166-ddd67580-8bf5-11eb-834a-ae4f1dd48129.png)

<br>

### 2. EC2에서 테스트 (Local k8s에서도 해도되나 EC2 권장)

현재 노드 상황 체크

```
$ kubectl get no
NAME                            STATUS   ROLES    AGE   VERSION
ip-172-20-37-137.ec2.internal   Ready    node     23m   v1.19.9
ip-172-20-43-68.ec2.internal    Ready    node     23m   v1.19.9
ip-172-20-51-247.ec2.internal   Ready    node     23m   v1.19.9
ip-172-20-52-12.ec2.internal    Ready    master   24m   v1.19.9
```

<br>

**k8s의 deployment 객체로 실행**

- deployment.yml 생성
- replicaset=1

```
$ mkdir yaml_files
$ cd yaml_files/
$ vi deployment.yml
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
 name: my-user-deployment
spec:   
 replicas: 1
 selector:
  matchLabels:
   app: my-user-app
 template:
  metadata:
   labels:
    app: my-user-app
  spec:
   containers:
   - name: my-user-ms
     image: edowon0623/my-user-service
     ports:
     - containerPort: 8080
```

파일 실행

```
$ kubectl apply -f deployment.yml 
deployment.apps/my-user-deployment created

$ kubectl get deployment
```

pod 어디에 설치되었는지 확인 후 테스트

```
$ kubectl get pod -o wide
NAME                                  READY   STATUS    RESTARTS   AGE     IP           NODE                           NOMINATED NODE   READINESS GATES
my-user-deployment-7cbc8995c6-964zv   1/1     Running   0          6m42s   100.96.3.3   ip-172-XX-XX-XX.ec2.internal   <none>           <none>
```

<br>

AWS콘솔에서 master 노드의 ip 주소 확인하여 `ssh` 명령어로 마스터 노드 접속

- 마스터 노드로 접속하며 ubuntu@ 뒤의 ip주소가 변경됨

```
ubuntu@ip-172-31-19-242:~/yaml_files$ ssh 3.239.158.155

ubuntu@ip-172-20-52-12:~$ 
```

<br>

아래 명령어 통해 얻은 IP 주소로 접속

```
ubuntu@ip-172-31-19-242:~/yaml_files$ kubectl get pod -o wide
NAME                                  READY   STATUS    RESTARTS   AGE     IP           NODE                            NOMINATED NODE   READINESS GATES
my-user-deployment-7cbc8995c6-bpkgd   1/1     Running   0          4h48m   100.96.4.2   ip-172-20-51-247.ec2.internal   <none>           <none>

ubuntu@ip-172-20-34-254:~$ curl 100.96.4.2:8080/users
[{"id":1,"name":"Kenneth","join_date":"2021-03-23T08:17:18.750+0000","password":"test1","ssn":"701010-1111111","posts":null},{"id":2,"name":"Alice","join_date":"2021-03-23T08:17:18.750+0000","password":"test2","ssn":"801111-2222222","posts":null},{"id":3,"name":"Elena","join_date":"2021-03-23T08:17:18.750+0000","password":"test3","ssn":"901313-1111111","posts":null}]
```

<br>

하지만 마스터 노드를 통해 100.96.3.3:8080/users로 접속하는 과정이 복잡하여 이를 간소화하고자 Service 등록

![Untitled Diagram (1)](https://user-images.githubusercontent.com/77096463/112254958-c0acb000-8ca4-11eb-9f74-08233e8e7c3d.png)

<br>

마스터노드가 아닌, kops와 kubectl이 설치된 클라이언트에서 진행

service.yml 파일 작성

```yaml
apiVersion: v1
kind: Service
metadata:
 name: my-user-app-service
spec:
 selector:
  app: my-user-app
 ports:
  - port: 8080
    targetPort: 8080
 type: NodePort
```
service.yml 파일 적용 및 포트번호 확인
```
ubuntu@ip-172-31-19-242:~/yaml_files$ kubectl apply -f service.yml
service/my-user-app-service created

ubuntu@ip-172-31-19-242:~/yaml_files$ kubectl get service
NAME                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes            ClusterIP   100.64.0.1     <none>        443/TCP          7m2s
my-user-app-service   NodePort    100.66.14.29   <none>        8080:32188/TCP   5s
```

외부에서의 접속을 위해 마스터노드의 보안그룹 인바운드 규칙에 포트번호 추가 

- TCP 포트번호 / 위치 무관

![image](https://user-images.githubusercontent.com/77096463/112236352-a1e9f180-8c83-11eb-87de-9be2975e62f8.png)

<br>

포스트맨으로 이동하여 http://master_ip:포트번호/users 접속 확인 > 이때 master_ip는 master instance의 public ip!

![image](https://user-images.githubusercontent.com/77096463/112236390-b0380d80-8c83-11eb-8234-4810e363454e.png)

<br>

지금까지의 내용을 namespace가 default인 것만 가져오기

```
$ kubectl get all --namespace=default
NAME                                      READY   STATUS    RESTARTS   AGE
pod/my-user-deployment-7cbc8995c6-bpkgd   1/1     Running   0          55m

NAME                          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes            ClusterIP   100.64.0.1     <none>        443/TCP          61m
service/my-user-app-service   NodePort    100.66.14.29   <none>        8080:32188/TCP   54m

NAME                                 READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/my-user-deployment   1/1     1            1           55m

NAME                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/my-user-deployment-7cbc8995c6   1         1         1       55m
```



